# 决策树

​	决策树是一类常见的机器学习方法，以二分类任务为例，我们希望给定训练数据集学得一个模型用以对新示例进行分类，这个把分类的任务，可看作对“当前样本属于正类吗？“的一个”决策“或”判断“过程。顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

## 优缺点

### 优点

1、速度快，计算量相对不大。

2、准确率高

3、直观，可以清晰显示哪些字段比较重要。

### 缺点

1、不减枝容易过拟合。

2、剪枝不当容易欠拟合。



## 如何选择最优划分属性

### 信息增益

​	“信息熵”是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为pk（k=1，2，3，...，|y|），则D的信息熵定义为:

​					
$$
Ent(D)=-\sum_{k=1}^{|y|}{p_k\log_2p_k}
$$
Ent(D)的值越小，则决策树的分支结点包含的样本越可能属于同一类别。

​	假定离散属性a有V个可能的取值{a1，a2，...，a V}，若使用a来对样本集进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为a V的样本，记为D V，我们可以根据上面的公式计算出D V的信息熵，再给分支结点赋予权重|D V|/|D|，即样本越多的分支结点的影响越大，于是可以计算出用属性a对样本集进行划分所获得的"信息增益"
$$
Gain(D,a)=End(D)-\sum_{v=1}^{V}{\frac{|D^V|}{|D|}Ent(D^v)}
$$
​	一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”（我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高）越大。因此，我们可以使用信息增益来选择最优划分属性，这就是ID3算法。

### 增益率

​	信息增益准则对可取数值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法不直接使用信息增益，而是使用“增益率”来选择最优划分属性，增益率定义为：
$$
Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum_{v=1}^{V}{\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}}
$$
这是属性a的固有值。属性a的可能去知数目越多（V越大），则IV(a)的值通常会越大。因为增益率会对可取数值数目较少的属性有所偏好，所以C4.5算法不是直接使用增益率来选择最优划分属性，而是先从候选划分属性中找出信息增益高出平均水平的，再从中选择增益率最高的。

### 基尼指数

​	CART决策树使用“基尼指数”来选择划分属性，数据集D的纯度可用基尼值来度量：
$$
Gini(D)=\sum_{k=1}^{|y|}{\sum_{k'≠k}^{}{p_kp_k'}}
$$
​	只管来说，Gini(D)反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，所以基尼值越小，数据集的纯度越高。

​	属性a的基尼指数定义为：
$$
Gini\_index(D,a)=\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Gini(D^v)}
$$
​	选择划分后基尼指数最小的属性作为最优划分属性。