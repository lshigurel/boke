# 残差神经网络（一）

## 为什么要用残差神经网络(解决了什么问题)?

​	当我们想要用卷积神经网络获取更多的特征时，我们往往是去加深神经网络的深度，但是这会带来一个梯度消失和梯度爆炸的问题。我们可以使用正则化和归一化来解决这一问题，但是神经网络又会出现一个退化的问题，即模型的准确率会达到饱和之后下降，损失升高，因此我们需要残差神经网络来解决这一问题。

## 如何解决退化问题？

![]( https://github.com/Shigurea/boke/blob/master/pic/6095626-49ac0caeb5525b93.png )

​	假定一个神经网络的输入是x，期望输出是H(x)，残差神经网络就是把输入值x作为输出的初始结果，改变学习的目标为F(x)=H(x)-x,不再是一个H(x)，而是H(x)-x，即是残差。神经网络学习一个残差相比于学习一个新的映射函数要简单得多，残差要更加敏感。

​	F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F'(5)=5.1，引入残差后是 H(5)=5.1，F(5)=H(5)-5 ，F(5)=0.1 。这里的F'和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射的输出增加了2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。



![]( https://github.com/Shigurea/boke/blob/master/pic/20180117221359702.png )

这是传统的神经网络与残差神经网络的结构对比，可以看到残差神经网络有很多skip connection，将输入直接传入后面的层，让其学习残差，这样既保证了信息的完整性，也简化了神经网络的学习难度。